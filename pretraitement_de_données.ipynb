{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4bf9af23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total: 853\n",
      "Résultat de la division: train=682, valid=171\n",
      "Terminé ✅\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Fichiers d'entrée et de sortie\n",
    "ground_truth_file = \"C:/Users/jguo/Documents/PURE/data/bert_simple_ground_truth.jsonl\"\n",
    "train_file = \"C:/Users/jguo/Documents/PURE/data/new_train.jsonl\"\n",
    "valid_file = \"C:/Users/jguo/Documents/PURE/data/new_valid.jsonl\"\n",
    "\n",
    "# Fixer la graine aléatoire pour assurer la reproductibilité\n",
    "random.seed(42)\n",
    "\n",
    "# Charger les données ground truth\n",
    "with open(ground_truth_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Nombre total: {len(data)}\")\n",
    "\n",
    "# Mélanger les données et diviser en 80/20\n",
    "random.shuffle(data)\n",
    "split = int(0.8 * len(data))\n",
    "train_data = data[:split]\n",
    "valid_data = data[split:]\n",
    "\n",
    "print(f\"Résultat de la division: train={len(train_data)}, valid={len(valid_data)}\")\n",
    "\n",
    "# Sauvegarder les fichiers de sortie\n",
    "with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in train_data:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(valid_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in valid_data:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Terminé ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a78e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 原始文件路径\n",
    "file_a = \"C:/Users/jguo/Documents/PURE/data/bert_simple_ground_truth.jsonl\"  # 包含你想“对比”的ID\n",
    "file_b = \"C:/Users/jguo/Documents/PURE/data/output.jsonl\"  # 需要清理的文件\n",
    "output_file = \"C:/Users/jguo/Documents/PURE/data/test_pure.jsonl\"  # 保存结果的路径\n",
    "\n",
    "# 第一步：收集 file_A 中所有的 id\n",
    "ids_in_a = set()\n",
    "with open(file_a, \"r\", encoding=\"utf-8\") as fa:\n",
    "    for line in fa:\n",
    "        data = json.loads(line)\n",
    "        ids_in_a.add(data[\"id\"])\n",
    "\n",
    "# 第二步：过滤 file_B 中的条目\n",
    "with open(file_b, \"r\", encoding=\"utf-8\") as fb, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fb:\n",
    "        data = json.loads(line)\n",
    "        if data[\"id\"] not in ids_in_a:\n",
    "            fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "389f589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Unidecode in c:\\users\\jguo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: dateparser in c:\\users\\jguo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\jguo\\appdata\\roaming\\python\\python313\\site-packages (from dateparser) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2024.2 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dateparser) (2025.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dateparser) (2025.7.34)\n",
      "Requirement already satisfied: tzlocal>=0.2 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dateparser) (5.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jguo\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\jguo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tzlocal>=0.2->dateparser) (2025.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\jguo\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!py -m pip install Unidecode dateparser\n",
    "# 或：python -m pip install Unidecode dateparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c6bc77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé ! Lignes: 682, Modifiées: 232\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import dateparser\n",
    "\n",
    "# 同时覆盖带/不带重音的月份\n",
    "MONTHS = [\n",
    "    \"janvier\",\n",
    "    \"février\", \"fevrier\",\n",
    "    \"mars\",\n",
    "    \"avril\",\n",
    "    \"mai\",\n",
    "    \"juin\",\n",
    "    \"juillet\",\n",
    "    \"août\", \"aout\",\n",
    "    \"septembre\",\n",
    "    \"octobre\",\n",
    "    \"novembre\",\n",
    "    \"décembre\", \"decembre\",\n",
    "]\n",
    "\n",
    "MONTHS_ALT = \"|\".join(MONTHS)\n",
    "DATE_PATTERN = re.compile(\n",
    "    rf\"\\b(?:1er|\\d{{1,2}})\\s+(?:{MONTHS_ALT})\\s+\\d{{4}}\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_full_dates(text: str):\n",
    "    \"\"\"\n",
    "    从原句中提取“完整日期表达”（例如：'1er février 1877'），\n",
    "    返回一个列表，元素为句子里的原始片段（保留重音与大小写）。\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    for m in DATE_PATTERN.finditer(text):\n",
    "        phrase = text[m.start():m.end()]\n",
    "        dt = dateparser.parse(\n",
    "            phrase,\n",
    "            languages=[\"fr\"],\n",
    "            settings={\"DATE_ORDER\": \"DMY\"}\n",
    "        )\n",
    "        if dt:\n",
    "            results.append(phrase.strip())\n",
    "    return results\n",
    "\n",
    "def replace_incomplete_dates(sent, triples):\n",
    "    \"\"\"\n",
    "    用句子中出现的“原始日期片段”替换 triples 里不完整/格式化的日期（如 '1877-02' 或 '1877-02-01'）。\n",
    "    \"\"\"\n",
    "    # 兜底：非字符串句子或 triples 非列表，安全返回\n",
    "    if not isinstance(sent, str):\n",
    "        return triples if isinstance(triples, list) else []\n",
    "\n",
    "    triples = triples if isinstance(triples, list) else []\n",
    "\n",
    "    # 1) 提取原句中的完整日期片段，并建立 (year, month) -> 原片段 的映射\n",
    "    full_dates = extract_full_dates(sent)\n",
    "    ym_to_phrase = {}\n",
    "    for phr in full_dates:\n",
    "        dt = dateparser.parse(\n",
    "            phr, languages=[\"fr\"],\n",
    "            settings={\"DATE_ORDER\": \"DMY\"}\n",
    "        )\n",
    "        if dt:\n",
    "            key = (f\"{dt.year}\", f\"{dt.month:02d}\")\n",
    "            # 若同一 (year, month) 出现多个，以最后一个为准；也可自定义优先级\n",
    "            ym_to_phrase[key] = phr\n",
    "\n",
    "    # 2) 逐个 triple 回写\n",
    "    for triple in triples:\n",
    "        obj = triple.get(\"obj\", \"\")\n",
    "        if not isinstance(obj, str):\n",
    "            continue\n",
    "\n",
    "        # 情况 A: YYYY-MM\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{2})\", obj)\n",
    "        if m:\n",
    "            year, month = m.group(1), m.group(2)\n",
    "            phr = ym_to_phrase.get((year, month))\n",
    "            if phr:\n",
    "                triple[\"obj\"] = phr\n",
    "            continue\n",
    "\n",
    "        # 情况 B: YYYY-MM-DD\n",
    "        m2 = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", obj)\n",
    "        if m2:\n",
    "            year, month, day = m2.group(1), m2.group(2), int(m2.group(3))\n",
    "            phr = ym_to_phrase.get((year, month))\n",
    "            if phr:\n",
    "                # day 出现在原片段里（如 '17' 或 '17 ' 等），或 1 对应 '1er'\n",
    "                day_str = str(day)\n",
    "                if (day_str in phr) or (f\"{day:02d}\" in phr) or (day == 1 and \"1er\" in phr):\n",
    "                    triple[\"obj\"] = phr\n",
    "            continue\n",
    "\n",
    "    return triples\n",
    "\n",
    "# ====== 批处理文件 ======\n",
    "input_file = \"C:/Users/jguo/Documents/PURE/data/new_train.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Documents/PURE/data/train_fixed.jsonl\"\n",
    "\n",
    "n_total = 0\n",
    "n_replaced = 0\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        d = json.loads(line)\n",
    "\n",
    "        sent = d.get(\"sent\")  # 某些行可能没有 'sent' 或值为 None\n",
    "        triples = d.get(\"triples\", [])\n",
    "\n",
    "        before = json.dumps(triples, ensure_ascii=False, sort_keys=True)\n",
    "        triples_fixed = replace_incomplete_dates(sent, triples)\n",
    "        after = json.dumps(triples_fixed, ensure_ascii=False, sort_keys=True)\n",
    "\n",
    "        if before != after:\n",
    "            n_replaced += 1\n",
    "\n",
    "        d[\"triples\"] = triples_fixed\n",
    "        fout.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "        n_total += 1\n",
    "\n",
    "print(f\"Traitement terminé ! Lignes: {n_total}, Modifiées: {n_replaced}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7eb25679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé !\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dateparser\n",
    "import re\n",
    "\n",
    "def fr_date_to_iso(date_str):\n",
    "    # Convertit une date en français (« 1er février 1877 », etc.) au format ISO (« 1877-02-01 »)\n",
    "    dt = dateparser.parse(date_str, languages=['fr'])\n",
    "    if dt:\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    return date_str\n",
    "\n",
    "def update_triples_date_obj(triples):\n",
    "    # Pour chaque triple, si l'objet est une date complète en français, on la remplace par le format ISO\n",
    "    pattern = r\"\\b(?:1er|\\d{1,2})\\s+[a-zéû]+(?:\\s+\\d{4})\"\n",
    "    for triple in triples:\n",
    "        obj = triple.get(\"obj\", \"\")\n",
    "        # Vérifie si l'objet est une date en français (« 1er février 1877 », « 21 mai 1956 », etc.)\n",
    "        if re.fullmatch(pattern, obj, re.IGNORECASE):\n",
    "            iso_date = fr_date_to_iso(obj)\n",
    "            triple[\"obj\"] = iso_date\n",
    "    return triples\n",
    "\n",
    "# ========== Traitement batch du fichier JSONL ==========\n",
    "input_file = \"C:/Users/jguo/Documents/PURE/data/train_fixed.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Documents/PURE/data/new_train.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        d = json.loads(line)\n",
    "        d[\"triples\"] = update_triples_date_obj(d[\"triples\"])\n",
    "        fout.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Traitement terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "60108396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tous les filtres appliqués. Entrées lues=171, sorties écrites=171. Fichier : C:/Users/jguo/Documents/PURE/data/valid_filtered.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# ===== 工具：不区分大小写/重音的包含判断 =====\n",
    "def _norm(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip().lower()\n",
    "    # 去重音\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "    # 统一空白\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _strip_voie_if_matches_sentence(text: str, sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    如果 text 以 'voie ' 开头，且去掉前缀的部分能在句子中找到（宽松匹配），\n",
    "    则返回去掉 'voie ' 的部分；否则返回原 text。\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    m = re.match(r\"^\\s*voie\\s+(.+)$\", text, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return text\n",
    "    tail = m.group(1).strip()\n",
    "    if not tail:\n",
    "        return text\n",
    "    if _norm(tail) in _norm(sentence):\n",
    "        return tail\n",
    "    return text\n",
    "\n",
    "# ===== 关系侧：哪些字段应当是“地名（整串）”，哪些字段是“类型（caractéristique géographique）” =====\n",
    "def sides_expect_names(rel: str):\n",
    "    \"\"\"\n",
    "    返回 (sub_is_name, obj_is_name)\n",
    "    \"\"\"\n",
    "    rel = (rel or \"\").strip()\n",
    "    # isLandmarkType: sub=地名, obj=类型\n",
    "    if rel == \"isLandmarkType\":\n",
    "        return True, False\n",
    "    # isLandmarkTypeOf: sub=类型, obj=地名\n",
    "    if rel in (\"isLandmarkTypeOf\", \"isLandmarkTypeOF\"):\n",
    "        return False, True\n",
    "    # 以下关系两边通常都是地名\n",
    "    if rel in (\"hasOldName\", \"hasNewName\", \"touches\", \"within\"):\n",
    "        return True, True\n",
    "    # 时间型关系：sub=地名, obj=时间\n",
    "    if rel in (\"hasGeometryChangeOn\", \"hasNameChangeOn\", \"isNumberedOn\",\n",
    "               \"isClassifiedOn\", \"disappearsOn\", \"appearsOn\", \"hasAppearedRelationOn\"):\n",
    "        return True, False\n",
    "    # 其他未知关系：保守认为 sub 是地名、obj 不一定\n",
    "    return True, False\n",
    "\n",
    "# ====== 运行 ======\n",
    "input_file = \"C:/Users/jguo/Documents/PURE/data/new_valid.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Documents/PURE/data/valid_filtered.jsonl\"\n",
    "\n",
    "n_in, n_out = 0, 0\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        n_in += 1\n",
    "        data = json.loads(line)\n",
    "\n",
    "        sent = data.get(\"sent\", \"\")  # 句子原文，用于匹配\n",
    "        triples = data.get(\"triples\", [])\n",
    "        new_triples = []\n",
    "\n",
    "        for triple in triples:\n",
    "            rel = triple.get(\"rel\", \"\")\n",
    "            sub = triple.get(\"sub\", \"\")\n",
    "            obj = triple.get(\"obj\", \"\")\n",
    "\n",
    "            # 1) 过滤掉 obj == \"noTime\"\n",
    "            if obj == \"noTime\":\n",
    "                continue\n",
    "\n",
    "            # 2) 过滤掉 hasNewName/hasOldName 中 sub==obj 的冗余\n",
    "            if rel in (\"hasNewName\", \"hasOldName\") and _norm(sub) == _norm(obj):\n",
    "                continue\n",
    "\n",
    "            if sub == \"municipality\":\n",
    "                continue\n",
    "\n",
    "            # 3) 纠正 'voie XXX' → 'XXX'（仅当该侧按关系应是“地名整串”且句子中能匹配）\n",
    "            sub_is_name, obj_is_name = sides_expect_names(rel)\n",
    "\n",
    "            if sub_is_name:\n",
    "                sub = _strip_voie_if_matches_sentence(sub, sent)\n",
    "            if obj_is_name:\n",
    "                obj = _strip_voie_if_matches_sentence(obj, sent)\n",
    "\n",
    "            # 写回三元组\n",
    "            triple[\"sub\"] = sub\n",
    "            triple[\"obj\"] = obj\n",
    "\n",
    "            new_triples.append(triple)\n",
    "\n",
    "        data[\"triples\"] = new_triples\n",
    "        fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "        n_out += 1\n",
    "\n",
    "print(f\"Tous les filtres appliqués. Entrées lues={n_in}, sorties écrites={n_out}. Fichier : {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a47c49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.30.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00269cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa1baf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dateparser in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\jguo\\appdata\\roaming\\python\\python310\\site-packages (from dateparser) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2024.2 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dateparser) (2025.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dateparser) (2024.11.6)\n",
      "Requirement already satisfied: tzlocal>=0.2 in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dateparser) (5.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jguo\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\jguo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tzlocal>=0.2->dateparser) (2025.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install dateparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b37e00d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity [Rue Cochin] not found in [boulevard de port-royal || Historique || Les rues des Trois Couronnes Saint-Marcel, Cochin, des Bourguignons, des Capucins, de Port-Royal, des Charbonniers Saint-Marcel, des Cendriers, du Champs des Capucins, l'impasse Hautefort et le Champ des Capucins ont été absorbés ou supprimés par ce boulevard]\n",
      "[INFO] Appariement flou 'rue Rouvet' ≈ 'rue roue', similarité=0.88\n",
      "Traitement terminé ! lus=682, écrits=682\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import difflib\n",
    "from unidecode import unidecode\n",
    "import dateparser\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/Users/jguo/Documents/PURE/camembert-base\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    return unidecode(text).lower().replace(\" \", \"\").replace(\"’\", \"\").replace(\"'\", \"\")\n",
    "\n",
    "# 在 normalize 下方加：\n",
    "def normalize_loose(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    s = unidecode(text).lower()\n",
    "    # 去掉所有非字母数字与斜杠（保留 J/16 这类代码）\n",
    "    s = re.sub(r\"[^a-z0-9/]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def char_to_token_span(tokens, text, char_start, char_end):\n",
    "    \"\"\"将字符区间映射到 SentencePiece token 区间。\"\"\"\n",
    "    curr_char = 0\n",
    "    start_token_idx = None\n",
    "    end_token_idx = None\n",
    "    for idx, token in enumerate(tokens):\n",
    "        token_str = token.replace(\"▁\", \" \")\n",
    "        token_str = token_str.strip()\n",
    "        # 跳过原文中的连续空白\n",
    "        while curr_char < len(text) and text[curr_char].isspace():\n",
    "            curr_char += 1\n",
    "        token_begin = curr_char\n",
    "        token_end = curr_char + len(token_str)\n",
    "        if start_token_idx is None and token_begin <= char_start < token_end:\n",
    "            start_token_idx = idx\n",
    "        if token_begin < char_end <= token_end:\n",
    "            end_token_idx = idx\n",
    "        curr_char = token_end\n",
    "    if start_token_idx is not None and end_token_idx is not None:\n",
    "        return start_token_idx, end_token_idx\n",
    "    return None, None\n",
    "\n",
    "def find_sublist_ci(lst, sub):\n",
    "    for i in range(len(lst) - len(sub) + 1):\n",
    "        if [t.lower() for t in lst[i:i+len(sub)]] == [t.lower() for t in sub]:\n",
    "            return i, i + len(sub) - 1\n",
    "    return -1, -1\n",
    "\n",
    "\n",
    "# ---------- 事件类型与时间抽取 ----------\n",
    "def tag_event_type(sent: str):\n",
    "    m = re.search(r\"\\|\\|\\s*([^\\|]+?)\\s*\\|\\|\", sent)\n",
    "    if m:\n",
    "        event = m.group(1).strip()\n",
    "        before_event = sent[:m.start()]\n",
    "        return event, before_event, m.start(), m.end()\n",
    "    return None, None, -1, -1\n",
    "\n",
    "def extract_time(sent: str):\n",
    "    \"\"\"\n",
    "    简单时间抽取：dd mois yyyy | yyyy-mm-dd | yyyy | C\\d{1,2}(e)?\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\d{1,2} [a-zéû]+ \\d{4}|\\d{4}-\\d{2}-\\d{2}|\\d{4}|C\\d{1,2}(e)?\", sent)\n",
    "    if m:\n",
    "        return m.group(), m.start(), m.end()\n",
    "    return None, -1, -1\n",
    "\n",
    "# ---------- 主找 span 函数 ----------\n",
    "def find_token_span(tokens, text, entity, entity_type=None):\n",
    "    # Sauter les entités \"thoroughfare\" et \"municipality\" sans aucun affichage\n",
    "    if entity_type in (\"thoroughfare\", \"municipality\"):\n",
    "        return None, None\n",
    "    # Appariement souple pour les codes de voies : \"voie AH/15\", \"AH/15\", etc.\n",
    "    if entity_type == \"landmark\":\n",
    "        code_pattern = r\"[A-Z]{1,3}/\\d{1,3}\"\n",
    "        entity_code = re.findall(code_pattern, entity)\n",
    "        if entity_code:\n",
    "            c = entity_code[0]\n",
    "            idx = text.find(c)\n",
    "            if idx != -1:\n",
    "                return char_to_token_span(tokens, text, idx, idx + len(c))\n",
    "        entity_words = entity.strip().split()\n",
    "        if len(entity_words) >= 2:\n",
    "            last_words = \" \".join(entity_words[-2:])\n",
    "            idx = text.lower().find(last_words.lower())\n",
    "            if idx != -1:\n",
    "                return char_to_token_span(tokens, text, idx, idx + len(last_words))\n",
    "            last_word = entity_words[-1]\n",
    "            idx2 = text.lower().find(last_word.lower())\n",
    "            if idx2 != -1:\n",
    "                left = max(0, idx2-6)\n",
    "                match = re.search(r\"(du|de|des|la|le)\\s+\" + re.escape(last_word), text[left:idx2+len(last_word)], re.IGNORECASE)\n",
    "                if match:\n",
    "                    span_start = left + match.start()\n",
    "                    span_end = left + match.end()\n",
    "                    return char_to_token_span(tokens, text, span_start, span_end)\n",
    "    if entity_type == \"date\":\n",
    "        century_match = re.fullmatch(r\"C(\\d{1,2})(e)?\", entity, re.IGNORECASE)\n",
    "        if century_match:\n",
    "            century_num = int(century_match.group(1))\n",
    "            roman_map = {\n",
    "                1: \"I\", 2: \"II\", 3: \"III\", 4: \"IV\", 5: \"V\", 6: \"VI\", 7: \"VII\", 8: \"VIII\", 9: \"IX\", 10: \"X\",\n",
    "                11: \"XI\", 12: \"XII\", 13: \"XIII\", 14: \"XIV\", 15: \"XV\", 16: \"XVI\", 17: \"XVII\", 18: \"XVIII\", 19: \"XIX\", 20: \"XX\"\n",
    "            }\n",
    "            roman = roman_map.get(century_num, \"\")\n",
    "            candidates = []\n",
    "            patterns = [\n",
    "                rf\"{century_num}e siècle\",\n",
    "                rf\"{century_num} siècle\",\n",
    "                rf\"{roman}e siècle\",\n",
    "                rf\"{roman} siècle\"\n",
    "            ]\n",
    "            for pat in patterns:\n",
    "                idx = text.lower().find(pat.lower())\n",
    "                if idx != -1:\n",
    "                    candidates.append((idx, idx + len(pat)))\n",
    "            if candidates:\n",
    "                char_start, char_end = candidates[0]\n",
    "                return char_to_token_span(tokens, text, char_start, char_end)\n",
    "        mois_fr = [\"janvier\", \"fevrier\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\",\n",
    "                   \"aout\", \"septembre\", \"octobre\", \"novembre\", \"decembre\"]\n",
    "        mois_fr_accent = [\"janvier\", \"février\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\",\n",
    "                          \"août\", \"septembre\", \"octobre\", \"novembre\", \"décembre\"]\n",
    "        m_full = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", entity)\n",
    "        m_month = re.fullmatch(r\"(\\d{4})-(\\d{2})\", entity)\n",
    "        m_year = re.fullmatch(r\"(\\d{4})\", entity)\n",
    "        year, month_idx, day = None, None, None\n",
    "        if m_full:\n",
    "            year = m_full.group(1)\n",
    "            month_idx = int(m_full.group(2)) - 1\n",
    "            day = int(m_full.group(3))\n",
    "        elif m_month:\n",
    "            year = m_month.group(1)\n",
    "            month_idx = int(m_month.group(2)) - 1\n",
    "        elif m_year:\n",
    "            year = m_year.group(1)\n",
    "        txt_norm = unidecode(text).lower()\n",
    "        candidates = []\n",
    "        if year and month_idx is not None:\n",
    "            for mois in [mois_fr[month_idx], mois_fr_accent[month_idx]]:\n",
    "                if day:\n",
    "                    regexs = [\n",
    "                        rf\"\\b{day}\\s+{mois}\\s+{year}\\b\",\n",
    "                        rf\"\\b{str(day).zfill(2)}\\s+{mois}\\s+{year}\\b\",\n",
    "                        rf\"\\b1er\\s+{mois}\\s+{year}\\b\" if day == 1 else \"\",\n",
    "                    ]\n",
    "                    for rgx in regexs:\n",
    "                        if not rgx: continue\n",
    "                        mobj = re.search(rgx, txt_norm)\n",
    "                        if mobj:\n",
    "                            candidates.append((mobj.start(), mobj.end()))\n",
    "                rgx = rf\"\\b{mois}\\s+{year}\\b\"\n",
    "                mobj = re.search(rgx, txt_norm)\n",
    "                if mobj:\n",
    "                    candidates.append((mobj.start(), mobj.end()))\n",
    "        elif year:\n",
    "            mobj = re.search(rf\"\\b{year}\\b\", txt_norm)\n",
    "            if mobj:\n",
    "                candidates.append((mobj.start(), mobj.end()))\n",
    "        if candidates:\n",
    "            candidates.sort()\n",
    "            char_start, char_end = candidates[0]\n",
    "            return char_to_token_span(tokens, unidecode(text), char_start, char_end)\n",
    "        if entity in text:\n",
    "            char_start = text.index(entity)\n",
    "            return char_to_token_span(tokens, text, char_start, char_start + len(entity))\n",
    "        if entity_type is not None and entity_type not in (\"thoroughfare\", \"municipality\"):\n",
    "            print(f\"Entity [{entity}] not found in [{text}]\")\n",
    "        return None, None\n",
    "\n",
    "    idx_raw = text.find(entity)\n",
    "    if idx_raw != -1:\n",
    "        char_start = idx_raw\n",
    "        char_end = idx_raw + len(entity)\n",
    "        return char_to_token_span(tokens, text, char_start, char_end)\n",
    "    entity_norm = normalize(entity)\n",
    "    text_norm = normalize(text)\n",
    "    if entity_norm in text_norm:\n",
    "        idx = text_norm.index(entity_norm)\n",
    "        best_start, best_end = None, None\n",
    "        for start in range(len(text)):\n",
    "            for end in range(start + len(entity), min(len(text), start + len(entity) + 8)):\n",
    "                frag = text[start:end]\n",
    "                if normalize(frag) == entity_norm:\n",
    "                    best_start, best_end = start, end\n",
    "                    break\n",
    "            if best_start is not None:\n",
    "                break\n",
    "        if best_start is not None and best_end is not None:\n",
    "            return char_to_token_span(tokens, text, best_start, best_end)\n",
    "    text_norm = normalize(text)\n",
    "    entity_norm = normalize(entity)\n",
    "    idx_norm = text_norm.find(entity_norm)\n",
    "    if idx_norm != -1:\n",
    "        best_start, best_end = None, None\n",
    "        for start in range(len(text)):\n",
    "            for end in range(start + 1, min(len(text), start + len(entity) + 8) + 1):\n",
    "                frag = text[start:end]\n",
    "                if normalize(frag) == entity_norm:\n",
    "                    best_start, best_end = start, end\n",
    "                    break\n",
    "            if best_start is not None:\n",
    "                break\n",
    "        if best_start is not None and best_end is not None:\n",
    "            return char_to_token_span(tokens, text, best_start, best_end)\n",
    "        else:\n",
    "            if entity_type is not None and entity_type not in (\"thoroughfare\", \"municipality\"):\n",
    "                print(f\"[AVERTISSEMENT] L'entité normalisée « {entity} » n'a pas été retrouvée dans le texte original « {text} ».\")\n",
    "        # —— 新增：宽松匹配（忽略标点/连字符等）\n",
    "    ent_loose = normalize_loose(entity)\n",
    "    txt_loose = normalize_loose(text)\n",
    "    if ent_loose and ent_loose in txt_loose:\n",
    "        # 找到一个最早的 loose 命中片段，映射回原文字符区间\n",
    "        # 简化实现：在原文中滑动窗口，找 normalize_loose 一致的最短片段\n",
    "        best_start = best_end = None\n",
    "        for start in range(len(text)):\n",
    "            for end in range(start + 1, min(len(text), start + len(entity) + 20) + 1):\n",
    "                if normalize_loose(text[start:end]) == ent_loose:\n",
    "                    best_start, best_end = start, end\n",
    "                    break\n",
    "            if best_start is not None:\n",
    "                break\n",
    "        if best_start is not None:\n",
    "            return char_to_token_span(tokens, text, best_start, best_end)\n",
    "\n",
    "    max_ratio = 0\n",
    "    best_start, best_end = None, None\n",
    "    for start in range(len(text)):\n",
    "        for end in range(start + max(2, len(entity) - 4), min(len(text), start + len(entity) + 8)):\n",
    "            frag = text[start:end]\n",
    "            frag_norm = normalize(frag)\n",
    "            if len(frag_norm) < 3:\n",
    "                continue\n",
    "            ratio = difflib.SequenceMatcher(None, frag_norm, entity_norm).ratio()\n",
    "            if ratio > 0.82 and ratio > max_ratio:\n",
    "                max_ratio = ratio\n",
    "                best_start, best_end = start, end\n",
    "    if best_start is not None and best_end is not None:\n",
    "        if entity_type not in (\"thoroughfare\", \"municipality\"):\n",
    "            print(f\"[INFO] Appariement flou '{entity}' ≈ '{text[best_start:best_end]}', similarité={max_ratio:.2f}\")\n",
    "        return char_to_token_span(tokens, text, best_start, best_end)\n",
    "    if entity_type is not None and entity_type not in (\"thoroughfare\", \"municipality\"):\n",
    "           print(f\"Entity [{entity}] not found in [{text}]\")\n",
    "    return None, None\n",
    "\n",
    "def looks_like_date(s: str) -> bool:\n",
    "    if not isinstance(s, str):\n",
    "        return False\n",
    "    s = s.strip()\n",
    "    if re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}\", s): return True\n",
    "    if re.fullmatch(r\"\\d{1,2} [a-zéû]+ \\d{4}\", s.lower()): return True\n",
    "    if re.fullmatch(r\"\\d{4}\", s): return True\n",
    "    if re.fullmatch(r\"C\\d{1,2}(e)?\", s, re.IGNORECASE): return True\n",
    "    return False\n",
    "\n",
    "def find_sublist(lst, sub):\n",
    "    \"\"\"Trouve le début et la fin d'une sous-liste dans une liste.\"\"\"\n",
    "    for i in range(len(lst) - len(sub) + 1):\n",
    "        if lst[i:i+len(sub)] == sub:\n",
    "            return i, i + len(sub) - 1\n",
    "    return -1, -1\n",
    "\n",
    "def _first_delim_left_end(tokens):\n",
    "    for i, t in enumerate(tokens):\n",
    "        if t in (\"|\", \"▁|\"):\n",
    "            return max(0, i - 1)\n",
    "    return len(tokens) - 1\n",
    "\n",
    "GEO_PREFIXES_LOWER = {\n",
    "    \"rue\", \"place\", \"boulevard\", \"quai\", \"impasse\", \"passage\",\n",
    "    \"allée\", \"avenue\", \"cours\", \"chemin\", \"square\", \"parvis\"   \n",
    "}\n",
    "\n",
    "def _is_geo_prefix_token(tok):\n",
    "    base = tok.replace(\"▁\", \"\").lower()\n",
    "    return base in GEO_PREFIXES_LOWER\n",
    "\n",
    "def _is_punct_token(tok):\n",
    "    base = tok.replace(\"▁\", \"\")\n",
    "    return base in {\",\", \"-\", \"–\", \"—\"}\n",
    "\n",
    "\n",
    "def build_ner(sent, tokens, triples=None):\n",
    "    ner = []\n",
    "    \n",
    "    def _prefer_primary_nom(spans):\n",
    "        \"\"\"\n",
    "        收敛 label='nom'：\n",
    "        - 选择最“主要”的 nom：起点最小者；若起点相同取跨度最大的。\n",
    "        - 删除其它被该主 nom 完全包含的 nom。\n",
    "        其他标签不动。\n",
    "        \"\"\"\n",
    "        noms = [(i, s, e) for i, (s, e, l) in enumerate(spans) if l == \"nom\"]\n",
    "        if not noms:\n",
    "            return spans\n",
    "\n",
    "        # 选主 nom：起点最小；若并列，跨度最大\n",
    "        s_keep, e_keep = sorted([(s, e) for _, s, e in noms],\n",
    "                                key=lambda se: (se[0], -(se[1]-se[0])))[0]\n",
    "\n",
    "        new = []\n",
    "        for s, e, l in spans:\n",
    "            if l == \"nom\" and not (s == s_keep and e == e_keep):\n",
    "                # 被主 nom 完全包含 → 丢弃\n",
    "                if s >= s_keep and e <= e_keep:\n",
    "                    continue\n",
    "            new.append([s, e, l])\n",
    "        return new\n",
    "\n",
    "    def _dedup_spans(spans):\n",
    "        \"\"\"按 (start,end,label) 精确去重，保留首次出现顺序。\"\"\"\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for s,e,l in spans:\n",
    "            key = (s,e,l)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            out.append([s,e,l])\n",
    "        return out\n",
    "\n",
    "    def _prefer_longest_nom_geo(spans):\n",
    "        \"\"\"\n",
    "        收敛 'nom géographique'：\n",
    "        - 若存在起点==0 的 NG，保留起点为0且跨度最长的那一个；\n",
    "        删除所有被其完全包含的 NG。\n",
    "        - 否则保留 NG 中跨度最长的那一个；\n",
    "        删除被其完全包含的其它 NG。\n",
    "        其他标签不动。\n",
    "        \"\"\"\n",
    "        ng_idxs = [(i, s, e) for i,(s,e,l) in enumerate(spans) if l == \"nom géographique\"]\n",
    "        if not ng_idxs:\n",
    "            return spans\n",
    "\n",
    "        # 候选：起点为 0 的 NG\n",
    "        zero = [(i,s,e) for i,s,e in ng_idxs if s == 0]\n",
    "        if zero:\n",
    "            keep_s, keep_e = max([(s,e) for _,s,e in zero], key=lambda se: (se[1]-se[0], -se[0]))\n",
    "        else:\n",
    "            keep_s, keep_e = max([(s,e) for _,s,e in ng_idxs], key=lambda se: (se[1]-se[0], -se[0]))\n",
    "\n",
    "        new = []\n",
    "        for s,e,l in spans:\n",
    "            if l == \"nom géographique\" and not (s == keep_s and e == keep_e):\n",
    "                # 被保留的 NG 完全包含 → 丢弃\n",
    "                if s >= keep_s and e <= keep_e:\n",
    "                    continue\n",
    "            new.append([s,e,l])\n",
    "        return new\n",
    "\n",
    "    # 1) 从 triples 收集需要当作“地名整串”的文本（按关系方向）\n",
    "    landmark_texts = []\n",
    "    if triples:\n",
    "        for t in triples:\n",
    "            rel = t.get(\"rel\", \"\")\n",
    "            sub = t.get(\"sub\", \"\")\n",
    "            obj = t.get(\"obj\", \"\")\n",
    "            if rel == \"isLandmarkType\":\n",
    "                if sub: landmark_texts.append(sub)       # sub = nom géographique\n",
    "            elif rel in (\"isLandmarkTypeOf\", \"isLandmarkTypeOF\"):\n",
    "                if obj: landmark_texts.append(obj)       # obj = nom géographique\n",
    "            elif rel in (\"hasOldName\", \"hasNewName\", \"touches\", \"within\"):\n",
    "                if sub: landmark_texts.append(sub)\n",
    "                if obj: landmark_texts.append(obj)\n",
    "            elif rel in (\"hasGeometryChangeOn\", \"hasNameChangeOn\", \"isNumberedOn\",\n",
    "                         \"isClassifiedOn\", \"disappearsOn\", \"appearsOn\", \"hasAppearedRelationOn\"):\n",
    "                if sub: landmark_texts.append(sub)\n",
    "            else:\n",
    "                if sub: landmark_texts.append(sub)\n",
    "\n",
    "    # 去重保序\n",
    "    seen = set(); lm_list = []\n",
    "    for s in landmark_texts:\n",
    "        if s and s not in seen:\n",
    "            seen.add(s); lm_list.append(s)\n",
    "\n",
    "    # 2) 可能的地物前缀（大小写不敏感）\n",
    "    geo_prefixes = [\n",
    "        \"Rue\", \"Place\", \"Boulevard\", \"Quai\", \"Impasse\", \"Passage\",\n",
    "        \"Allée\", \"Avenue\", \"Cours\", \"Chemin\", \"Square\"\n",
    "    ]\n",
    "\n",
    "    # 3) 为每个地名串产出三类标签\n",
    "        # 3) 为每个地名串产出三类标签\n",
    "    for loc_text in lm_list:\n",
    "        loc_tokens = tokenizer.tokenize(loc_text)\n",
    "        s, e = find_sublist_ci(tokens, loc_tokens)\n",
    "        if s == -1:\n",
    "            # 子序列不等（大小写/重音/分词差异），退回字符级查找再映射到 token\n",
    "            s, e = find_token_span(tokens, sent, loc_text, entity_type=\"landmark\")\n",
    "        if s in (None, -1) or e in (None, -1):\n",
    "            continue\n",
    "\n",
    "        # —— 新增：若 loc_text 命中的是“专名部分”，且左邻是地物前缀，则把整串起点左扩到前缀\n",
    "        # 例如：tokens = [\"▁rue\",\"▁valent\",\"in\",\"▁ha\",\"ü\",\"y\",...]\n",
    "        # loc_text = \"Valentin Haüy\" 命中 [1,5]，左邻 tokens[0] = \"rue\"（前缀），应当合并为 [0,5]\n",
    "        if s - 1 >= 0 and _is_geo_prefix_token(tokens[s - 1]):\n",
    "            # 先标前缀\n",
    "            ner.append([s - 1, s - 1, \"caractéristique géographique\"])\n",
    "            # 整串改为包含前缀\n",
    "            s0 = s - 1\n",
    "            ner.append([s0, e, \"nom géographique\"])\n",
    "            # nom 就是专名（保持从原 s 开始）\n",
    "            ner.append([s, e, \"nom\"])\n",
    "            continue  # 已完成本条 loc_text 处理\n",
    "\n",
    "        # —— 原有逻辑：在命中范围内再找“最长的内部前缀”\n",
    "        ner.append([s, e, \"nom géographique\"])\n",
    "        best = None\n",
    "        for pref in [\"Rue\",\"Place\",\"Boulevard\",\"Quai\",\"Impasse\",\"Passage\",\"Allée\",\"Avenue\",\"Cours\",\"Chemin\",\"Square\"]:\n",
    "            p_tok = tokenizer.tokenize(pref)\n",
    "            L = len(p_tok)\n",
    "            if s + L - 1 <= e and [t.lower() for t in tokens[s:s+L]] == [t.lower() for t in p_tok]:\n",
    "                if best is None or L > best[2]:\n",
    "                    best = (s, s+L-1, L)\n",
    "        if best:\n",
    "            ner.append([best[0], best[1], \"caractéristique géographique\"])\n",
    "            nom_start = best[1] + 1\n",
    "        else:\n",
    "            nom_start = s\n",
    "\n",
    "        # —— 新规则：只跳过“紧跟前缀的”法语虚词，再遇到首个实词就停止跳过\n",
    "        INITIAL_STOPS = {\"de\", \"du\", \"des\", \"la\", \"le\"}\n",
    "        while nom_start <= e:\n",
    "            tok = tokens[nom_start].replace(\"▁\", \"\").lower()\n",
    "            # 合并 l' / d' 的切分情况，比如 \"l\" + \"'\" 或 \"d\" + \"'\"\n",
    "            if tok in {\"l\", \"d\"} and nom_start + 1 <= e and tokens[nom_start+1] == \"'\":\n",
    "                nom_start += 2\n",
    "                continue\n",
    "            if tok in INITIAL_STOPS:\n",
    "                nom_start += 1\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        if nom_start <= e:\n",
    "            ner.append([nom_start, e, \"nom\"])\n",
    "\n",
    "    # === 句首兜底：若当前还没有“从 0 开始”的 nom géographique”，而句首看起来就是地名（含多前缀） ===\n",
    "    if not any(lbl == \"nom géographique\" and s == 0 for s, e, lbl in ner):\n",
    "        if len(tokens) > 1 and _is_geo_prefix_token(tokens[0]):\n",
    "            end0 = _first_delim_left_end(tokens)\n",
    "            if end0 >= 0 and end0 > 0:\n",
    "                # 整串：标题区\n",
    "                ner.append([0, end0, \"nom géographique\"])\n",
    "\n",
    "                # 所有前缀单词都标注为 'caractéristique géographique'\n",
    "                prefix_positions = []\n",
    "                for i in range(0, end0 + 1):\n",
    "                    if _is_geo_prefix_token(tokens[i]):\n",
    "                        ner.append([i, i, \"caractéristique géographique\"])\n",
    "                        prefix_positions.append(i)\n",
    "\n",
    "                # nom：从最后一个前缀后的第一个非虚词开始，到标题末尾（去掉收尾标点）\n",
    "                nom_start = 0\n",
    "                if prefix_positions:\n",
    "                    nom_start = max(prefix_positions) + 1\n",
    "\n",
    "                # 跳过开头标点\n",
    "                def _is_punct_token(tok):\n",
    "                    base = tok.replace(\"▁\", \"\")\n",
    "                    return base in {\",\", \"-\", \"–\", \"—\"}\n",
    "\n",
    "                while nom_start <= end0 and _is_punct_token(tokens[nom_start]):\n",
    "                    nom_start += 1\n",
    "\n",
    "                # 只跳过“紧跟前缀的”法语虚词\n",
    "                INITIAL_STOPS = {\"de\", \"du\", \"des\", \"la\", \"le\"}\n",
    "                while nom_start <= end0:\n",
    "                    tok = tokens[nom_start].replace(\"▁\", \"\").lower()\n",
    "                    if tok in {\"l\", \"d\"} and nom_start + 1 <= end0 and tokens[nom_start+1] == \"'\":\n",
    "                        nom_start += 2\n",
    "                        continue\n",
    "                    if tok in INITIAL_STOPS:\n",
    "                        nom_start += 1\n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "                # 末尾去掉标点\n",
    "                nom_end = end0\n",
    "                while nom_end >= nom_start and _is_punct_token(tokens[nom_end]):\n",
    "                    nom_end -= 1\n",
    "\n",
    "                if nom_start <= nom_end:\n",
    "                    ner.append([nom_start, nom_end, \"nom\"])\n",
    "\n",
    "\n",
    "    # 4) EventType / Time 保留你的逻辑\n",
    "    event, _, _, _ = tag_event_type(sent)\n",
    "    if event:\n",
    "        ev_tokens = tokenizer.tokenize(event)\n",
    "        s2, e2 = find_sublist_ci(tokens, ev_tokens)\n",
    "        if s2 != -1:\n",
    "            ner.append([s2, e2, \"EventType\"])\n",
    "\n",
    "    time_str, t_start, _ = extract_time(sent)\n",
    "    if time_str:\n",
    "        before = sent[:t_start]\n",
    "        b_tok = tokenizer.tokenize(before)\n",
    "        t_tok = tokenizer.tokenize(time_str)\n",
    "        ner.append([len(b_tok), len(b_tok)+len(t_tok)-1, \"Time\"])\n",
    "    ner = _dedup_spans(ner)\n",
    "    ner = _prefer_longest_nom_geo(ner)   # 你已添加，用来收敛 NG\n",
    "    ner = _prefer_primary_nom(ner)       # ← 新增：收敛 nom，保留最左且尽量长的那个\n",
    "    return ner\n",
    "\n",
    "\n",
    "\n",
    "def build_relations(sent, tokens, ner, triples):\n",
    "    relations = []\n",
    "    def _first_delim_token_index(tokens):\n",
    "        for i, t in enumerate(tokens):\n",
    "            if t in (\"|\", \"▁|\"):\n",
    "                return i\n",
    "        return len(tokens)  # 没有分隔符就当整句\n",
    "\n",
    "    def get_span_from_ner_pref(entity, label, prefer_before_idx=None):\n",
    "        \"\"\"先按文本归一化匹配；若有多个候选，优先 start < prefer_before_idx。否则取跨度最大的。\"\"\"\n",
    "        ent_norm = normalize(entity or \"\")\n",
    "        candidates = []\n",
    "        for start, end, ner_label in ner:\n",
    "            if ner_label != label:\n",
    "                continue\n",
    "            ner_text = \"\".join(tokens[start:end+1]).replace(\"▁\", \" \").strip()\n",
    "            if normalize(ner_text) == ent_norm:\n",
    "                candidates.append((start, end))\n",
    "        if not candidates:\n",
    "            # 回退：拿该 label 的全部候选中，优先标题区\n",
    "            candidates = [(s, e) for (s, e, l) in ner if l == label]\n",
    "        if not candidates:\n",
    "            return None\n",
    "        if prefer_before_idx is not None:\n",
    "            title_side = [(s, e) for (s, e) in candidates if s < prefer_before_idx]\n",
    "            if title_side:\n",
    "                candidates = title_side\n",
    "        # 取跨度最大的\n",
    "        candidates.sort(key=lambda x: (x[1]-x[0], -x[0]), reverse=True)\n",
    "        return candidates[0]\n",
    "\n",
    "    def _pick_longest_span(ner, label):\n",
    "        c = [(s, e) for (s, e, l) in ner if l == label]\n",
    "        if not c: return None\n",
    "        c.sort(key=lambda x: (x[1]-x[0], -x[0]), reverse=True)\n",
    "        return c[0]\n",
    "\n",
    "    def _leading_name_span(tokens, ner):\n",
    "        \"\"\"返回句首的 nom géographique（如果 NER 已有）或直接用 0..第一个'|'之前。\"\"\"\n",
    "        span = _pick_longest_span(ner, \"nom géographique\")\n",
    "        if span and span[0] == 0:\n",
    "            return span\n",
    "        end0 = _first_delim_left_end(tokens)\n",
    "        if end0 >= 0 and end0 > 0:\n",
    "            return (0, end0)\n",
    "        return None\n",
    "\n",
    "    def get_span_from_ner(entity, label):\n",
    "        best_span = None\n",
    "        ent_norm = normalize(entity or \"\")\n",
    "        for start, end, ner_label in ner:\n",
    "            if ner_label != label:\n",
    "                continue\n",
    "            ner_text = \"\".join(tokens[start:end+1]).replace(\"▁\", \" \").strip()\n",
    "            if normalize(ner_text) == ent_norm:\n",
    "                return (start, end)\n",
    "            if best_span is None or (end - start) > (best_span[1] - best_span[0]):\n",
    "                best_span = (start, end)\n",
    "        return best_span\n",
    "\n",
    "    for triple in (triples or []):\n",
    "        sub_type = triple.get(\"sub_type\", \"landmark\")\n",
    "        obj_type = \"date\" if looks_like_date(triple.get(\"obj\")) else triple.get(\"obj_type\", \"landmark\")\n",
    "        if sub_type in (\"thoroughfare\", \"municipality\") or obj_type in (\"thoroughfare\", \"municipality\"):\n",
    "            continue\n",
    "\n",
    "        rel = triple.get(\"rel\")\n",
    "        sub_span = obj_span = None\n",
    "\n",
    "        rel_norm = rel\n",
    "        if rel == \"isLandmarkTypeOF\":\n",
    "            rel_norm = \"isLandmarkTypeOf\"\n",
    "\n",
    "        elif rel_norm == \"isLandmarkType\":\n",
    "            # 规则：sub = nom géographique, obj = caractéristique géographique（标题优先）\n",
    "            delim_idx = _first_delim_token_index(tokens)\n",
    "\n",
    "            sub_span = get_span_from_ner_pref(triple.get(\"sub\"), \"nom géographique\",\n",
    "                                            prefer_before_idx=delim_idx)\n",
    "            obj_span = get_span_from_ner_pref(triple.get(\"obj\"), \"caractéristique géographique\",\n",
    "                                            prefer_before_idx=delim_idx)\n",
    "\n",
    "            # 兜底（字符级宽松查找 → token 映射）\n",
    "            if sub_span is None:\n",
    "                sub_span = find_token_span(tokens, sent, triple.get(\"sub\"), entity_type=\"landmark\")\n",
    "            if obj_span is None:\n",
    "                obj_span = find_token_span(tokens, sent, triple.get(\"obj\"), entity_type=\"landmark\")\n",
    "\n",
    "        elif rel_norm == \"isLandmarkTypeOf\":\n",
    "            # sub = caractéristique géographique, obj = nom géographique\n",
    "            delim_idx = _first_delim_token_index(tokens)\n",
    "            sub_span = get_span_from_ner_pref(triple.get(\"sub\"), \"caractéristique géographique\", prefer_before_idx=delim_idx)\n",
    "            obj_span = get_span_from_ner_pref(triple.get(\"obj\"), \"nom géographique\", prefer_before_idx=delim_idx)\n",
    "\n",
    "            if sub_span is None:\n",
    "                sub_span = find_token_span(tokens, sent, triple.get(\"sub\"), entity_type=\"landmark\")\n",
    "            if obj_span is None:\n",
    "                obj_span = find_token_span(tokens, sent, triple.get(\"obj\"), entity_type=\"landmark\")\n",
    "\n",
    "\n",
    "        elif rel_norm in (\"hasGeometryChangeOn\", \"hasNameChangeOn\", \"isNumberedOn\",\n",
    "                          \"isClassifiedOn\", \"disappearsOn\", \"appearsOn\", \"hasAppearedRelationOn\"):\n",
    "            sub_span = get_span_from_ner(triple.get(\"sub\"), \"nom géographique\")\n",
    "            obj_span = get_span_from_ner(triple.get(\"obj\"), \"Time\")\n",
    "            if sub_span is None:\n",
    "                sub_span = find_token_span(tokens, sent, triple.get(\"sub\"), entity_type=\"landmark\")\n",
    "            if obj_span is None:\n",
    "                obj_span = find_token_span(tokens, sent, triple.get(\"obj\"), entity_type=\"date\")\n",
    "\n",
    "        elif rel_norm in (\"hasOldName\", \"hasNewName\"):\n",
    "            # 目标：sub = 句首的“nom géographique”（主街名），obj = 旧/新名称（也是地名串）\n",
    "            # 先按文本精确找\n",
    "            sub_span = get_span_from_ner(triple.get(\"sub\"), \"nom géographique\")\n",
    "            obj_span = get_span_from_ner(triple.get(\"obj\"), \"nom géographique\")\n",
    "\n",
    "            # sub 找不到 → 回退到句首地名\n",
    "            if sub_span is None:\n",
    "                sub_span = _leading_name_span(tokens, ner)\n",
    "\n",
    "            # obj 找不到 → 再用通用 finder 兜底\n",
    "            if obj_span is None:\n",
    "                obj_span = find_token_span(tokens, sent, triple.get(\"obj\"), entity_type=\"landmark\")\n",
    "\n",
    "            # 仍然没有就再用“NER最长 nom géographique”作为 obj（极端兜底）\n",
    "            if obj_span is None:\n",
    "                obj_span = _pick_longest_span(ner, \"nom géographique\")\n",
    "\n",
    "        elif rel_norm in (\"touches\", \"within\"):\n",
    "            sub_span = get_span_from_ner(triple.get(\"sub\"), \"nom géographique\")\n",
    "            obj_span = get_span_from_ner(triple.get(\"obj\"), \"nom géographique\")\n",
    "            if sub_span is None:\n",
    "                sub_span = find_token_span(tokens, sent, triple.get(\"sub\"), entity_type=\"landmark\")\n",
    "            if obj_span is None:\n",
    "                obj_span = find_token_span(tokens, sent, triple.get(\"obj\"), entity_type=\"landmark\")\n",
    "\n",
    "        else:\n",
    "            # 其他关系：地名-时间作兜底\n",
    "            sub_span = get_span_from_ner(triple.get(\"sub\"), \"nom géographique\")\n",
    "            obj_span = get_span_from_ner(triple.get(\"obj\"), \"Time\")\n",
    "            if sub_span is None:\n",
    "                sub_span = find_token_span(tokens, sent, triple.get(\"sub\"), entity_type=\"landmark\")\n",
    "            if obj_span is None:\n",
    "                # 如果 obj 不是时间，看作地名兜底\n",
    "                etype = \"date\" if looks_like_date(triple.get(\"obj\")) else \"landmark\"\n",
    "                obj_span = find_token_span(tokens, sent, triple.get(\"obj\"), entity_type=etype)\n",
    "\n",
    "        if sub_span and sub_span[0] is not None and obj_span and obj_span[0] is not None:\n",
    "            relations.append([sub_span[0], sub_span[1], obj_span[0], obj_span[1], rel])\n",
    "    \n",
    "\n",
    "    return relations\n",
    "\n",
    "\n",
    "def process_one_line(d):\n",
    "    sent = d.get(\"sent\")\n",
    "    if not isinstance(sent, str) or not sent.strip():\n",
    "        # 跳过非字符串或空句子的样本，避免 tokenizer 抛 TypeError\n",
    "        return None\n",
    "\n",
    "    doc_key = d.get(\"id\", \"\")\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "\n",
    "    ner = build_ner(sent, tokens, d.get(\"triples\", []))\n",
    "    relations = build_relations(sent, tokens, ner, d.get(\"triples\", []))\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"doc_key\": doc_key,\n",
    "        \"dataset\": \"évolutions d'événements\",\n",
    "        \"sentences\": [tokens],\n",
    "        \"ner\": [ner],\n",
    "        \"relations\": [relations],\n",
    "    }\n",
    "\n",
    "# ====== 批处理 ======\n",
    "input_file = \"C:/Users/jguo/Documents/PURE/data/train_filtered.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Documents/PURE/data/train_pure.jsonl\"\n",
    "\n",
    "n_in, n_out = 0, 0\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        n_in += 1\n",
    "        data = json.loads(line)\n",
    "        if not isinstance(data.get(\"sent\"), str) or not data.get(\"sent\").strip():\n",
    "            print(\"[SKIP]\", data.get(\"id\"), \"sent=\", repr(data.get(\"sent\")))\n",
    "\n",
    "        ex = process_one_line(data)\n",
    "        if ex is None:  # 跳过非法样本\n",
    "            continue\n",
    "        fout.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "        n_out += 1\n",
    "        # 放在批处理 for 循环里的 json.loads(line) 后面\n",
    "        \n",
    "\n",
    "print(f\"Traitement terminé ! lus={n_in}, écrits={n_out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
